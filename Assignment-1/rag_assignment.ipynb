{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Assignment\n",
    "\n",
    "## Problem Statement\n",
    "Build a Question-Answering system using RAG architecture that can answer questions based on PDF documents. The system will:\n",
    "1. Load and process PDF documents\n",
    "2. Create vector embeddings for efficient retrieval\n",
    "3. Use Google's Gemini model for generating accurate responses based on retrieved context\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset / Knowledge Source\n",
    "- **Type of Data**: PDF documents\n",
    "- **Data Source**: Sample PDF documents (can be research papers, manuals, or any text-based PDFs)\n",
    "- **Location**: `data/` folder\n",
    "\n",
    "---\n",
    "\n",
    "## RAG Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           RAG PIPELINE ARCHITECTURE                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                              INDEXING PHASE\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PDF     â”‚â”€â”€â”€â–¶â”‚   Text       â”‚â”€â”€â”€â–¶â”‚   Text      â”‚â”€â”€â”€â–¶â”‚   Embedding    â”‚\n",
    "â”‚  Docs    â”‚    â”‚   Extraction â”‚    â”‚   Chunking  â”‚    â”‚   Generation   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                               â”‚\n",
    "                                                               â–¼\n",
    "                                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                       â”‚  FAISS Vector  â”‚\n",
    "                                                       â”‚    Store       â”‚\n",
    "                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                               â”‚\n",
    "                              RETRIEVAL PHASE                  â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  User    â”‚â”€â”€â”€â–¶â”‚   Query      â”‚â”€â”€â”€â–¶â”‚  Similarity â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚  Query   â”‚    â”‚   Embedding  â”‚    â”‚   Search    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                           â–¼\n",
    "                              GENERATION PHASE\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚   Top-K Relevant Chunks â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PROMPT TEMPLATE                           â”‚\n",
    "â”‚  Context: {retrieved_chunks}                                 â”‚\n",
    "â”‚  Question: {user_query}                                      â”‚\n",
    "â”‚  Answer based on context only.                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚   Google Gemini LLM     â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                       â”‚   Generated Response    â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "print(\"âœ… All dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load PDF Documents\n",
    "\n",
    "We use `PyPDFLoader` to extract text from PDF files. The `DirectoryLoader` allows loading multiple PDFs from a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "def load_documents(data_path):\n",
    "    \"\"\"\n",
    "    Load all PDF documents from the specified directory.\n",
    "    Returns a list of Document objects with page content and metadata.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        data_path,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents(DATA_PATH)\n",
    "print(f\"âœ… Loaded {len(documents)} pages from PDF documents\")\n",
    "\n",
    "# Display sample content\n",
    "if documents:\n",
    "    print(f\"\\nğŸ“„ Sample content from first page:\")\n",
    "    print(f\"Source: {documents[0].metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content preview: {documents[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text Chunking Strategy\n",
    "\n",
    "### Chunking Configuration\n",
    "- **Chunk Size**: 1000 characters\n",
    "- **Chunk Overlap**: 200 characters\n",
    "\n",
    "### Reason for Chosen Strategy\n",
    "1. **RecursiveCharacterTextSplitter**: Splits text hierarchically (paragraphs â†’ sentences â†’ words) preserving semantic coherence\n",
    "2. **Chunk Size (1000)**: Optimal balance between:\n",
    "   - Enough context for meaningful retrieval\n",
    "   - Not too large to dilute relevance\n",
    "   - Fits within embedding model context limits\n",
    "3. **Overlap (200)**: Ensures continuity between chunks, preventing loss of context at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text chunking configuration\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def chunk_documents(documents, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    Uses RecursiveCharacterTextSplitter for semantic preservation.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Hierarchical splitting\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = chunk_documents(documents)\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {len(documents)} pages\")\n",
    "print(f\"\\nğŸ“Š Chunking Statistics:\")\n",
    "print(f\"   - Chunk Size: {CHUNK_SIZE} characters\")\n",
    "print(f\"   - Chunk Overlap: {CHUNK_OVERLAP} characters\")\n",
    "print(f\"   - Average chunks per page: {len(chunks)/max(len(documents),1):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Embeddings\n",
    "\n",
    "### Embedding Model Details\n",
    "- **Model**: Google's `models/embedding-001`\n",
    "- **Dimension**: 768\n",
    "\n",
    "### Reason for Selection\n",
    "1. **High Quality**: Google's embedding model provides state-of-the-art semantic understanding\n",
    "2. **Integration**: Seamless integration with Google Gemini for generation\n",
    "3. **Cost-Effective**: Free tier available for development\n",
    "4. **Multilingual**: Supports multiple languages out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "def get_embeddings():\n",
    "    \"\"\"\n",
    "    Initialize Google Generative AI Embeddings.\n",
    "    Uses the embedding-001 model for text embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\"\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "print(\"âœ… Embedding model initialized: models/embedding-001\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_embedding = embeddings.embed_query(\"test query\")\n",
    "print(f\"   - Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create FAISS Vector Store\n",
    "\n",
    "### Vector Database Details\n",
    "- **Database**: FAISS (Facebook AI Similarity Search)\n",
    "\n",
    "### Reason for Selection\n",
    "1. **Speed**: Optimized for fast similarity search on CPU\n",
    "2. **Memory Efficient**: Uses efficient indexing structures\n",
    "3. **Local Storage**: No external database required\n",
    "4. **Scalability**: Handles millions of vectors efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store configuration\n",
    "VECTOR_STORE_PATH = \"vectorstore/faiss_index\"\n",
    "\n",
    "def create_vector_store(chunks, embeddings, store_path=VECTOR_STORE_PATH):\n",
    "    \"\"\"\n",
    "    Create FAISS vector store from document chunks.\n",
    "    Saves the index locally for reuse.\n",
    "    \"\"\"\n",
    "    # Create vector store\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Save locally\n",
    "    vector_store.save_local(store_path)\n",
    "    print(f\"âœ… Vector store saved to: {store_path}\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def load_vector_store(embeddings, store_path=VECTOR_STORE_PATH):\n",
    "    \"\"\"\n",
    "    Load existing FAISS vector store from disk.\n",
    "    \"\"\"\n",
    "    vector_store = FAISS.load_local(\n",
    "        store_path,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "# Create or load vector store\n",
    "if chunks:\n",
    "    vector_store = create_vector_store(chunks, embeddings)\n",
    "    print(f\"âœ… Vector store created with {len(chunks)} vectors\")\n",
    "else:\n",
    "    print(\"âš ï¸ No chunks to index. Please add PDF files to the data/ folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setup Retrieval Chain\n",
    "\n",
    "Configure the retriever and LLM for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure retriever\n",
    "TOP_K = 4  # Number of relevant chunks to retrieve\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": TOP_K}\n",
    ")\n",
    "\n",
    "print(f\"âœ… Retriever configured with top-{TOP_K} similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Google Gemini)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,  # Lower temperature for factual responses\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM initialized: gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that answers questions based on the provided context.\n",
    "Use ONLY the information from the context to answer the question.\n",
    "If the answer is not found in the context, say \"I cannot find this information in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear, concise answer based on the context above.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt template configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Stuff all retrieved docs into context\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Query Function\n",
    "\n",
    "Helper function to query the RAG system and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question: str, show_sources: bool = True):\n",
    "    \"\"\"\n",
    "    Query the RAG system with a question.\n",
    "    Returns the answer and optionally shows source documents.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“ Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get response\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Answer:\")\n",
    "    print(response[\"result\"])\n",
    "    \n",
    "    if show_sources and \"source_documents\" in response:\n",
    "        print(f\"\\nğŸ“š Sources ({len(response['source_documents'])} documents):\")\n",
    "        for i, doc in enumerate(response[\"source_documents\"], 1):\n",
    "            source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "            page = doc.metadata.get(\"page\", \"N/A\")\n",
    "            print(f\"   {i}. {source} (Page: {page})\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Queries\n",
    "\n",
    "Testing the RAG system with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 1\n",
    "response1 = query_rag(\"What is the main topic of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 2\n",
    "response2 = query_rag(\"Can you summarize the key points discussed?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 3\n",
    "response3 = query_rag(\"What are the conclusions or recommendations mentioned?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "\n",
    "### 1. Better Chunking Strategies\n",
    "- **Semantic Chunking**: Use sentence embeddings to create semantically coherent chunks\n",
    "- **Document Structure Aware**: Respect headers, sections, and paragraphs\n",
    "- **Sliding Window with Overlap**: Variable overlap based on content density\n",
    "\n",
    "### 2. Reranking / Hybrid Search\n",
    "- **Cross-Encoder Reranking**: Use a cross-encoder model to rerank retrieved chunks\n",
    "- **Hybrid Search**: Combine dense (embedding) and sparse (BM25) retrieval\n",
    "- **Maximal Marginal Relevance (MMR)**: Diversify retrieved results\n",
    "\n",
    "### 3. Metadata Filtering\n",
    "- **Filter by Document**: Allow querying specific documents\n",
    "- **Filter by Date**: Filter based on document date/version\n",
    "- **Filter by Section**: Target specific sections of documents\n",
    "\n",
    "### 4. UI Integration\n",
    "- **Streamlit Dashboard**: Interactive web interface (see `app.py`)\n",
    "- **Chat History**: Maintain conversation context\n",
    "- **Document Upload**: Allow users to upload their own PDFs\n",
    "\n",
    "### 5. Additional Enhancements\n",
    "- **Query Expansion**: Rephrase queries for better retrieval\n",
    "- **Multi-modal RAG**: Support images, tables from PDFs\n",
    "- **Caching**: Cache embeddings and responses for efficiency\n",
    "- **Evaluation Metrics**: Implement RAGAS for quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Choice | Reason |\n",
    "|-----------|--------|--------|\n",
    "| Document Loader | PyPDFLoader | Reliable PDF text extraction |\n",
    "| Text Splitter | RecursiveCharacterTextSplitter | Semantic coherence preservation |\n",
    "| Chunk Size | 1000 chars | Balance between context and precision |\n",
    "| Chunk Overlap | 200 chars | Continuity between chunks |\n",
    "| Embedding Model | Google embedding-001 | High quality, free tier available |\n",
    "| Vector Store | FAISS | Fast, local, memory efficient |\n",
    "| LLM | Gemini 1.5 Flash | Cost-effective, fast responses |\n",
    "| Chain Type | Stuff | Simple, effective for small context |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
